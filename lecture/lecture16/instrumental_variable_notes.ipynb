{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrumental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review and introduction to linear structural models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To briefly recap what we have learnt about observational studies so far, we\n",
    "1. Defined a *non-parametric* superpopulation model, i.e. a distribution for $(X,Z,Y(0),Y(1))$, with the estimand of interest, the ATE defiend to be $\\tau = \\mathbb{E}[Y(1) - Y(0)]$.\n",
    "2. Showed that it is impossible to estimate this unless we have further assumptions.\n",
    "3. Introduced the unconfoundedness assumption $\\{Y(0),Y(1)\\}\\perp\\!\\!\\perp Z~|~X$.\n",
    "4. Introduced the outcome regression and inverse propensity score weighting methods to estimate $\\tau$ under this assumption.\n",
    "\n",
    "In some fields like economics, it is typical to work with *structural models*, which places some restrictions on the joint distribution of all the variables, but also make it easier to estimate the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear structural model (LSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**: We will work with the *linear structural model*\n",
    "$$\n",
    "Y = \\alpha + \\tau Z + \\beta^TX + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is exogenous (i.e. has mean zero, and is independent of $Z$ and $X$). We sometimes further assume that $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$, but this is typically not necessary. $Y$ is still our outcome variable, $Z$ is our treatment variable of interest (typically binary), while $X$ is a confounder variable. \n",
    "\n",
    "*Remark: We usually add the further structural equation $Z = f(X, \\delta)$ where $\\delta$ is an exogenous noise variable, and $f$ encodes the structural relationship between $X$ and $Z$, but all you need to know for now is that we allow for $\\textrm{Cov}(Z,X) \\neq 0$.*\n",
    "\n",
    "This is not the same as the *linear model* that we have seen in the section on GLMs, and in previous classes. In those settings, we were assuming a model about the joint distribution of the variables, which is a statement about associations. On the other hand, a linear structural model encodes an assumption about *intervention*. It assumes that if for unit $i$, if we could set $Z_i = 1$, we will observe $Y_i(1) = \\tau + \\beta^TX_i + \\epsilon_i$, and if we could set $Z_i = 0$, we will observe $Y_i(0) = \\beta^TX_i + \\epsilon_i$. If $Z$ is not binary, then there will be a potential outcome for each possible value of $Z$. This is a subtle but important point, that also situates the linear structural model as a special case of the potential outcomes framework.\n",
    "\n",
    "From this, we see that the average treatment effect in this model is $\\tau$, and furthermore, that the individual treatment effect for every unit is\n",
    "\n",
    "$$Y_i(1) - Y_i(0) = \\tau.$$\n",
    "\n",
    "In other words, the treatment effect is constant. One may check that unconfoundedness holds, and $\\tau$ can thus be estimated using the methods covered in the previous lecture. On the other hand, we can also run a linear regresssion of $Y \\sim Z + X$.\n",
    "\n",
    "### Causal graphs and LSMs\n",
    "\n",
    "Apart from the causal effect of $Z$ on $Y$, the linear structural model also does something new from before. It asserts the causal relationships between the other variables, i.e. it tells us how $Z$ and $Y$ change if we manipulate $X$. The above linear structural model can be represented graphically as follows:\n",
    "\n",
    "<img src=\"causal_graph1.png\" align=\"center\"/>\n",
    "\n",
    "The arrows from $X$ into $Z$ and $Y$ asserts that $X$ causes both $Z$ and $Y$ (i.e. intervening on $Z$ changes the values of $X$ and $Y$), while the arrow from $Z$ into $Y$ asserts that $Z$ causes $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Confounding and omitted variable bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not observe $X$, then we are in trouble because of *confounding*. Let's see what happens when we try to regress $Y \\sim Z$, defining $\\tau_{OLS}$ be the solution of the least squares problem $\\min_{\\tau,\\alpha} \\mathbb{E}[(\\alpha + \\tau Z - Y)^2]$. We then get\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\tau_{OLS} & = \\frac{\\text{Cov}(Y,Z)}{\\text{Var}(Z)} \\\\\n",
    "    & = \\frac{\\text{Cov}(\\alpha + \\tau Z + \\beta^TX + \\epsilon,Z)}{\\text{Var}(Z)} \\\\\n",
    "    & = \\frac{\\text{Cov}(\\tau Z,Z)}{\\text{Var}(Z)} + \\frac{\\text{Cov}(\\beta^TX,Z)}{\\text{Var}(Z)} \\\\\n",
    "    & = \\tau + \\beta^T\\frac{\\text{Cov}(X,Z)}{\\text{Var}(Z)}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "There is hence the bias term $\\beta^T\\frac{\\text{Cov}(X,Z)}{\\text{Var}(Z)}$, which we call the *omitted variable bias*.\n",
    "\n",
    "*Remark: $\\frac{\\text{Cov}(Y,Z)}{\\text{Var}(Z)}$ is the infinite population version of the typical formula $\\hat{\\tau}_{OLS} = (Z^TZ)^{-1}Z^TY$, where we now use $Z$ and $Y$ to denote matrices/vectors.*\n",
    "\n",
    "Here are some examples of treatments $Z$, outcomes $Y$, and their possible confounders $X$. \n",
    "\n",
    "**Example 1:** $Z$ is health insurance, $Y$ is health outcomes, $X$ is socioeconomic background.\n",
    "\n",
    "**Example 2:** $Z$ is military service, $Y$ is salary, $X$ is socioeconomic background.\n",
    "\n",
    "**Example 3:** $Z$ is family size, $Y$ is mother's employment, $X$ is socioeconomic background.\n",
    "\n",
    "**Example 4:** $Z$ is years of schooling, $Y$ is salary, $X$ is socioeconomic background.\n",
    "\n",
    "**Example 5:** $Z$ is smoking, $Y$ is lung cancer, $X$ is socioeconomic background.\n",
    "\n",
    "**Why can't we just adjust for confounding?** Having such confounders is problematic because in order to avoid omitted variable bias, we need to have observed them, and added them to our regression (collection of such data may not always be feasible for a number of reasons.) Furthermore, there could always be *other* confounders that we are unaware of, which leaves our causal conclusions under an inescapable cloud of doubt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introducing instrumental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a middle way between a randomized experiment and assuming unconfoundedness, which is sometimes unrealistic?\n",
    "\n",
    "One way forward is when nature provides us with a \"partial\" natural experiment, i.e. we have a truly randomized \"instrument\" that injects an element of partial randomization into the treatment variable of interest. This is the idea of instrumental variables. We will first define the concept mathematically, and then illustrate what it means for a few examples.\n",
    "\n",
    "**Definition:** Assume the linear structural model defined above. We further assume a variable $W$ such that $Z = \\alpha' + \\gamma W + (\\beta')^TX + \\delta$, with $\\gamma \\neq 0$ (relevance), $W$ independent of $X$, $\\delta$ and $\\epsilon$ (exogeneity). Such a $W$ is called an *instrumental variable*.\n",
    "\n",
    "*Remark:* This replaces the earlier equation from before that $Z = f(X,\\delta)$.\n",
    "\n",
    "Let us now see how to use $W$ to identify the ATE $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    \\textrm{Cov}(Y,W) & = \\textrm{Cov}(\\alpha + \\tau Z + \\beta^TX + \\epsilon,W) \\\\\n",
    "    & = \\tau \\textrm{Cov}(Z,W) \\\\\n",
    "    & = \\tau \\textrm{Cov}(\\alpha' + \\gamma W + (\\beta')^TX + \\delta, W) \\\\\n",
    "    & = \\tau\\gamma \\textrm{Var}(W).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where the second equality follows from the exogeneity of $W$. Meanwhile, a similar computation with $Z$ and $W$ gives us\n",
    "$$\n",
    "\\textrm{Cov}(Z,W) = \\gamma\\textrm{Var}(W).\n",
    "$$\n",
    "\n",
    "Putting everything together gives\n",
    "$$\n",
    "\\tau = \\frac{\\frac{\\textrm{Cov}(Y,W)}{\\textrm{Var}(W)}}{\\frac{\\textrm{Cov}(Z,W)}{\\textrm{Var}(W)}}.\n",
    "$$\n",
    "\n",
    "In other words, $\\tau$ is the ratio between the (infinite population) regression coefficient of $W$ on $Y$, and that of $W$ on $Z$.\n",
    "\n",
    "This motivates the **instrumental variable estimator** of the ATE in finite samples:\n",
    "\n",
    "$$\n",
    "\\hat{\\tau}_{IV} = \\frac{(W^TW)^{-1}W^TY}{(W^TW)^{-1}W^TZ},\n",
    "$$\n",
    "\n",
    "where again, abusing notation, $W$, $Z$ and $Y$ refer to the vectors of observations. If $\\alpha' = 0$, then this is a plug-in estimator of $\\tau$, and is consistent.\n",
    "\n",
    "**Further interpretation for binary $W$:** When $W$ is binary, we can show that\n",
    "$$\n",
    "\\tau = \\frac{\\mathbb{E}[Y|W=1] - \\mathbb{E}[Y|W=0]}{\\mathbb{E}[Z|W=1] - \\mathbb{E}[Z|W=0]}.\n",
    "$$\n",
    "Hence, we can think of IV as measuring the ratio of the prima facie treatment effect of $W$ on $Y$ and that of $W$ on $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal graph for instrumental variables\n",
    "\n",
    "The relationships between $W, Z, X$, and $Y$ can be represented as the following causal graph:\n",
    "\n",
    "<img src=\"causal_graph2.png\" align=\"center\"/>\n",
    "\n",
    "How to read this graph:\n",
    "- The arrow from $W$ into $Z$ shows that $W$ has a causal effect on $Z$\n",
    "- The absence of any arrow into $W$ means that $W$ is exogeneous, i.e. no variable in the diagram causes $W$, and in particular $W$ is independent of $X$.\n",
    "- The absence of an arrow from $W$ into $Y$ means that the only effect of $W$ on $Y$ is through $Z$.\n",
    "- We shaded in $W$, $Z$ and $Y$ because these nodes are observed, but $X$ is unshaded because it is latent (unobserved).\n",
    "\n",
    "**An important point:** We do not need to know or even be aware of what $X$ is in order for instrumental variables to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Examples of instrumental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us revisit the examples introduced in the previous section.\n",
    "\n",
    "**Example 1:** $Z$ is health insurance, $Y$ is health outcomes, $X$ is socioeconomic background. Baicker et al. (2013) used the 2008 expansion of Medicaid in Oregon via lottery (we talked about this in Lecture 14). The instrument $W$ here was the lottery assignment. We previously talked about why this was an imperfect experiment because of compliance reasons (only a fraction of individuals who won the lottery enrolled into Medicaid), so IV provides a way of overcoming this limitation.\n",
    "\n",
    "**Example 2:** $Z$ is military service, $Y$ is salary, $X$ is socioeconomic background. Angrist (1990) used the Vietnam era draft lottery as the instrument $W$, and found that among white veterans, there was a 15% drop in earnings compared to non-veterans.\n",
    "\n",
    "**Example 3:** $Z$ is family size, $Y$ is mother's employment, $X$ is socioeconomic background. Angrist and Evans (1998) used sibling sex decomposition as the IV. This is plausible because of the pseudo randomization of the sibling sex composition. This is based on the fact that parents in the US with two children of the same sex are more likely to have a third child than those parents with two children of different sex.\n",
    "\n",
    "**Example 4:** $Z$ is years of schooling, $Y$ is salary, $X$ is socioeconomic background. Card (1993) used geographical variation in college proximity as the instrumental variable.\n",
    "\n",
    "**Example 5:** $Z$ is smoking, $Y$ is lung cancer, $X$ is socioeconomic background. Unfortunately, hard to come up with an IV here.\n",
    "\n",
    "As we see in these examples, sometimes you need to be quite ingenious to come up with an appropriate instrumental variable. Joshua Angrist, who is named in many of these examples, is a master at this. He is a professor of economics at MIT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extensions\n",
    "\n",
    "### Multiple treatments / instruments, and two-stage least squares.\n",
    "\n",
    "So far, we have considered scalar treatment and instrumental variables $Z$ and $W$. It is also possible to consider vector-valued instruments and treatments. To generalize IV to this setting, we need to recast the IV estimator in the previous sections as follows.\n",
    "\n",
    "First define the conditional expectation $\\tilde{Z} = \\mathbb{E}[Z|W]$, and observe that $\\tilde{Z} = \\alpha' + W\\gamma$.\n",
    "\n",
    "If we regress $Y$ on $\\tilde{Z}$, the regression coefficient we obtain is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\textrm{Cov}(\\tilde{Z},Y)}{\\textrm{Var}(\\tilde{Z})} & = \\frac{\\textrm{Cov}(\\tilde{Z}, \\alpha + \\tau Z + \\beta^TX + \\epsilon)}{\\textrm{Var}(\\tilde{Z})} \\\\\n",
    "& = \\frac{\\textrm{Cov}(\\tilde{Z}, \\tau Z)}{\\textrm{Var}(\\tilde{Z})} \\\\\n",
    "& = \\tau\\frac{\\textrm{Cov}(\\tilde{Z},  Z)}{\\textrm{Var}(\\tilde{Z})} \\\\\n",
    "& = \\tau.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here, the 2nd equality holds because $W$ is independent of all $X$ and $\\epsilon$, while the 4th equality holds because of a property of conditional expectations (one can also check this by hand by expanding out $Z = \\alpha' + \\gamma W + (\\beta')^TX + \\delta$.)\n",
    "\n",
    "In finite samples, we thus arrive at the following algorithm:\n",
    "\n",
    "**Two-stage least squares algorithm (2SLS):**\n",
    "- Step 1: Regress $Z$ on $W$ to get $\\tilde{Z} = W\\hat{\\gamma} = W(W^TW)^{-1}W^TZ$.\n",
    "- Step 2: Regress $Y$ on $\\tilde{Z}$ to get $\\hat{\\tau}_{2SLS} = (\\tilde{Z}^T\\tilde{Z})^{-1}\\tilde{Z}^TY$.\n",
    "\n",
    "For the scalar setting, it is easy to see that $\\hat{\\tau}_{2SLS} = \\hat{\\tau}_{IV}$, but the benefit of this formulation is that it directly applies for vector-valued $Z$ and $W$.\n",
    "\n",
    "### A non-parametric perspective on instrumental variables (optional)\n",
    "\n",
    "In this notebook, we have introduced instrumental variables in the context of structural linear models. In an amazing coincidence, for binary treatment $Z$, the expression\n",
    "$$\n",
    "\\tau = \\frac{\\mathbb{E}[Y|W=1] - \\mathbb{E}[Y|W=0]}{\\mathbb{E}[Z|W=1] - \\mathbb{E}[Z|W=0]}.\n",
    "$$\n",
    "has a meaning beyond the linear model setting. This is the subject of this groundbreaking [paper](https://www.jstor.org/stable/2291629?seq=1) by Angrist and Imbens in 1996."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
